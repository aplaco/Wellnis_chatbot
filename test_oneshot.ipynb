{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==((====))==  Unsloth 2025.5.7: Fast Llama patching. Transformers: 4.51.3.\n",
      "   \\\\   /|    NVIDIA GeForce RTX 3090. Num GPUs = 1. Max memory: 23.691 GB. Platform: Linux.\n",
      "O^O/ \\_/ \\    Torch: 2.4.0+cu121. CUDA: 8.6. CUDA Toolkit: 12.1. Triton: 3.0.0\n",
      "\\        /    Bfloat16 = TRUE. FA [Xformers = 0.0.27.post2+cu118. FA2 = False]\n",
      " \"-____-\"     Free license: http://github.com/unslothai/unsloth\n",
      "Unsloth: Fast downloading is enabled - ignore downloading bars which are red colored!\n"
     ]
    }
   ],
   "source": [
    "# Unsloth 라이브러리에서 FastLanguageModel을 임포트\n",
    "from unsloth import FastLanguageModel\n",
    "import torch\n",
    "\n",
    "max_seq_length = 2048 # 최대 시퀀스 길이를 설정 ( 텍스트의 최대 길이를 지정)\n",
    "dtype = None  # 자동 감지를 위해 None 설정. Tesla T4는 Float16, Ampere+는 Bfloat16 사용. 모델의 파라미터를 저장할 데이터 타입\n",
    "load_in_4bit = True  # 메모리 사용량을 줄이기 위해 4비트 양자화 사용. 다만 양자화에 따른 손실이 있어서 필요에 따라 False로 설정 가능\n",
    "\n",
    "# 사전 학습된 모델과 토크나이저 로드\n",
    "model, tokenizer = FastLanguageModel.from_pretrained(\n",
    "    model_name = \"unsloth/Meta-Llama-3.1-8B\",  # 사용할 모델 이름\n",
    "    max_seq_length = max_seq_length,         # 설정한 최대 시퀀스 길이\n",
    "    dtype = dtype,                           # 데이터 타입 설정\n",
    "    load_in_4bit = load_in_4bit,             # 4비트 양자화 여부\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# PEFT(파라미터 효율적 파인튜닝) 모델 설정\n",
    "model = FastLanguageModel.get_peft_model(\n",
    "    model,\n",
    "    r = 16,  # LoRA 랭크 설정. 8, 16, 32, 64, 128 권장. r 값이 클수록 모델이 더 많은 정보를 학습할 수 있지만, 너무 크면 메모리를 많이 사용\n",
    "    target_modules = [\"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\",\n",
    "                      \"gate_proj\", \"up_proj\", \"down_proj\"],  # PEFT 적용할 모듈 목록. 모델의 특정 부분(모듈)에만 학습\n",
    "    lora_alpha = 16,        # LoRA 알파 설정 LoRA라는 기술이 얼마나 강하게 작용할지 조절\n",
    "    lora_dropout = 0,       # LoRA 드롭아웃 설정. 0으로 최적화\n",
    "    bias = \"none\",          # 바이어스 설정. \"none\"으로 최적화\n",
    "\n",
    "    # \"unsloth\" 사용 시 VRAM 절약 및 배치 사이즈 2배 증가\n",
    "    # 학습할 때 메모리를 절약하는 방법을 사용하는 설정\n",
    "    use_gradient_checkpointing = \"unsloth\",  # 매우 긴 컨텍스트를 위해 \"unsloth\" 설정\n",
    "    random_state = 3407,    # 랜덤 시드 설정\n",
    "    use_rslora = False,     # 랭크 안정화 LoRA 사용 여부\n",
    "    loftq_config = None,    # LoftQ 설정 (사용하지 않음)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'<|end_of_text|>'"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 모델에게 주어질 텍스트의 형식을 정의\n",
    "alpaca_prompt = \"\"\"Below is an instruction that describes a task. Write a response that appropriately completes the request.\n",
    "\n",
    "### Instruction:\n",
    "{}\n",
    "\n",
    "### Response:\n",
    "{}\"\"\"\n",
    "\n",
    "# 맨윗줄: 모델에게 앞으로 제공될 지시사항을 기반으로 적절한 응답을 작성하라고 말함.\n",
    "# 모델이 어떤 작업을 수행해야 하는지 명확히 이해할 수 있도록 도와줌\n",
    "# Instruction:은 지시사항이 제공될 부분\n",
    "# Response:는 모델이 생성해야 할 응답이 제공될 부분\n",
    "\n",
    "# EOS 토큰 가져오기 (생성 종료를 위해 필요)\n",
    "EOS_TOKEN = tokenizer.eos_token  # 반드시 EOS_TOKEN을 추가해야 함\n",
    "EOS_TOKEN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 프롬프트 포맷팅 함수 정의\n",
    "def formatting_prompts_func(examples):\n",
    "    instructions = examples[\"유저\"]  # 데이터셋의 'instruction' 필드\n",
    "    outputs      = examples[\"챗봇\"]       # 데이터셋의 'output' 필드\n",
    "    texts = []\n",
    "    for instruction, output in zip(instructions, outputs):\n",
    "                                                           # EOS_TOKEN을 추가하지 않으면 생성이 무한히 계속됨\n",
    "        text = alpaca_prompt.format(instruction, output) + EOS_TOKEN  # 프롬프트 형식에 맞게 텍스트 생성\n",
    "        texts.append(text)\n",
    "    return { \"text\" : texts, }  # 'text' 필드로 반환\n",
    "\n",
    "#from datasets import load_dataset  # Hugging Face datasets 라이브러리 임포트\n",
    "\n",
    "# 데이터셋 로드 (Teddy Lee의 QA 데이터셋 미니 버전)\n",
    "#dataset = load_dataset(\"teddylee777/QA-Dataset-mini\", split = \"train\")\n",
    "\n",
    "# 프롬프트 포맷팅 함수 적용하여 데이터셋 변환\n",
    "#dataset = dataset.map(formatting_prompts_func, batched = True,)\n",
    "\n",
    "from datasets import load_dataset\n",
    "dataset = load_dataset( \"csv\", data_files = \"/home/alpaco/chat_bot/wellnis.csv\", split = \"train\")\n",
    "dataset = dataset.map(formatting_prompts_func, batched=True)\n",
    "\n",
    "# 예제 엑셀 형태로 데이터셋을 만든다면?\n",
    "# from datasets import load_dataset\n",
    "# dataset = load_dataset( \"csv\", data_files = \"data.csv\", split = \"train\")\n",
    "# dataset = dataset.map(formatting_prompts_func, batched=True)\n",
    "#\n",
    "#"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Below is an instruction that describes a task. Write a response that appropriately completes the request.\\n\\n### Instruction:\\n다 제 잘못이에요.\\n\\n### Response:\\n낚시를 하듯이 조금 더 먼 곳을 보고, 조금 더 마음을 비우면 편안해질 거예요.<|end_of_text|>',\n",
       " 'Below is an instruction that describes a task. Write a response that appropriately completes the request.\\n\\n### Instruction:\\n어렸을 때부터 엄마보다는 아빠랑 더 친했어요.\\n\\n### Response:\\n그러시군요. 아버지에 대해 더 들려주세요.<|end_of_text|>',\n",
       " 'Below is an instruction that describes a task. Write a response that appropriately completes the request.\\n\\n### Instruction:\\n몸무게도 늘고요.\\n\\n### Response:\\n체중이 단기간에 많이 늘어나면 건강에 좋지 않다고 해요.<|end_of_text|>',\n",
       " 'Below is an instruction that describes a task. Write a response that appropriately completes the request.\\n\\n### Instruction:\\n눈물이 날 거 같더라고.\\n\\n### Response:\\n감수성이 풍부하다는 증거 같아요. 눈물이 많은 사람 중에 나쁜 사람은 없잖아요.<|end_of_text|>',\n",
       " 'Below is an instruction that describes a task. Write a response that appropriately completes the request.\\n\\n### Instruction:\\n회사를 옮기면서 직급도 올라서, 지금은 과장이에요.\\n\\n### Response:\\n그렇군요. 새로운 환경이라 적응이 필요하겠어요.<|end_of_text|>']"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset['text'][:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "==((====))==  Unsloth - 2x faster free finetuning | Num GPUs used = 1\n",
      "   \\\\   /|    Num examples = 857 | Num Epochs = 1 | Total steps = 60\n",
      "O^O/ \\_/ \\    Batch size per device = 2 | Gradient accumulation steps = 4\n",
      "\\        /    Data Parallel GPUs = 1 | Total batch size (2 x 4 x 1) = 8\n",
      " \"-____-\"     Trainable parameters = 41,943,040/8,000,000,000 (0.52% trained)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='60' max='60' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [60/60 01:26, Epoch 0/1]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>3.274200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>3.394200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>3.408800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>3.288600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>3.069200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6</td>\n",
       "      <td>2.557600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7</td>\n",
       "      <td>2.456700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8</td>\n",
       "      <td>2.317200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9</td>\n",
       "      <td>2.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10</td>\n",
       "      <td>1.810600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>11</td>\n",
       "      <td>1.776400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>12</td>\n",
       "      <td>1.719400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>13</td>\n",
       "      <td>1.723100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>14</td>\n",
       "      <td>1.628200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>15</td>\n",
       "      <td>1.644300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>16</td>\n",
       "      <td>1.730100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>17</td>\n",
       "      <td>1.782100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>18</td>\n",
       "      <td>1.506400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>19</td>\n",
       "      <td>1.660700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>20</td>\n",
       "      <td>1.510300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>21</td>\n",
       "      <td>1.627300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>22</td>\n",
       "      <td>1.469800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>23</td>\n",
       "      <td>1.422100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>24</td>\n",
       "      <td>1.594500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>25</td>\n",
       "      <td>1.455000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>26</td>\n",
       "      <td>1.478300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>27</td>\n",
       "      <td>1.359900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>28</td>\n",
       "      <td>1.367100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>29</td>\n",
       "      <td>1.532700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>30</td>\n",
       "      <td>1.424300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>31</td>\n",
       "      <td>1.551600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>32</td>\n",
       "      <td>1.428300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>33</td>\n",
       "      <td>1.492900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>34</td>\n",
       "      <td>1.542600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>35</td>\n",
       "      <td>1.612400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>36</td>\n",
       "      <td>1.471100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>37</td>\n",
       "      <td>1.550500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>38</td>\n",
       "      <td>1.544800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>39</td>\n",
       "      <td>1.444400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>40</td>\n",
       "      <td>1.682000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>41</td>\n",
       "      <td>1.434800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>42</td>\n",
       "      <td>1.462900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>43</td>\n",
       "      <td>1.565900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>44</td>\n",
       "      <td>1.471200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>45</td>\n",
       "      <td>1.377900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>46</td>\n",
       "      <td>1.392300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>47</td>\n",
       "      <td>1.416100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>48</td>\n",
       "      <td>1.359600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>49</td>\n",
       "      <td>1.590900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>50</td>\n",
       "      <td>1.374200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>51</td>\n",
       "      <td>1.360100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>52</td>\n",
       "      <td>1.432100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>53</td>\n",
       "      <td>1.504700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>54</td>\n",
       "      <td>1.488200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>55</td>\n",
       "      <td>1.470900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>56</td>\n",
       "      <td>1.383900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>57</td>\n",
       "      <td>1.558900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>58</td>\n",
       "      <td>1.432800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>59</td>\n",
       "      <td>1.509600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>60</td>\n",
       "      <td>1.446500</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "## 학습 설정\n",
    "from trl import SFTTrainer  # TRL 라이브러리에서 SFTTrainer 임포트\n",
    "from transformers import TrainingArguments  # 트랜스포머 라이브러리에서 TrainingArguments 임포트\n",
    "from unsloth import is_bfloat16_supported  # BFloat16 지원 여부 확인 함수 임포트\n",
    "\n",
    "# SFTTrainer 인스턴스 생성\n",
    "trainer = SFTTrainer(\n",
    "    model = model,                           # 학습할 모델\n",
    "    tokenizer = tokenizer,                   # 사용할 토크나이저\n",
    "    train_dataset = dataset,                 # 학습할 데이터셋 ★★★★★★★★\n",
    "    dataset_text_field = \"text\",             # 데이터셋의 텍스트 필드 이름 ★★★★★★★★\n",
    "    max_seq_length = max_seq_length,         # 최대 시퀀스 길이\n",
    "    dataset_num_proc = 2,                    # 데이터셋 전처리에 사용할 프로세스 수 cpu\n",
    "    packing = False,                         # 짧은 시퀀스의 경우 packing을 비활성화 (학습 속도 5배 향상 가능)\n",
    "    args = TrainingArguments(\n",
    "        per_device_train_batch_size = 2,     # 디바이스 당 배치 사이즈\n",
    "        gradient_accumulation_steps = 4,     # 그래디언트 누적 단계 수\n",
    "        warmup_steps = 5,                     # 워밍업 스텝 수\n",
    "        # num_train_epochs = 1,               # 전체 학습 에폭 수 설정 가능\n",
    "        max_steps = 60,                       # 최대 학습 스텝 수\n",
    "        learning_rate = 2e-4,                 # 학습률\n",
    "        fp16 = not is_bfloat16_supported(),   # BFloat16 지원 여부에 따라 FP16 사용\n",
    "        bf16 = is_bfloat16_supported(),       # BFloat16 사용 여부\n",
    "        logging_steps = 1,                    # 로깅 빈도\n",
    "        optim = \"adamw_8bit\",                  # 옵티마이저 설정 (8비트 AdamW)\n",
    "        weight_decay = 0.01,                  # 가중치 감쇠\n",
    "        lr_scheduler_type = \"linear\",         # 학습률 스케줄러 타입\n",
    "        seed = 3407,                           # 랜덤 시드 설정\n",
    "        output_dir = \"outputs\",                # 출력 디렉토리\n",
    "    ),\n",
    ")\n",
    "\n",
    "## 학습 실행\n",
    "trainer_stats = trainer.train()  # 모델 학습 시작 # 3d6e4b9a0ddcd9edfafad59dcde0fd8c666954fd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('wellnis/tokenizer_config.json',\n",
       " 'wellnis/special_tokens_map.json',\n",
       " 'wellnis/tokenizer.json')"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 모델 저장 로컬폴더에다가 저장하는 방식\n",
    "model.save_pretrained(\"wellnis\")  # Local saving\n",
    "tokenizer.save_pretrained(\"wellnis\")\n",
    "\n",
    "# 이 이외에 허깅페이스나 다른 hub에 push해서 저장하는 방법이 있음\n",
    "# 다만, 업로드 속도와 다운로드 속도를 고려해야함."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==((====))==  Unsloth 2025.5.7: Fast Llama patching. Transformers: 4.51.3.\n",
      "   \\\\   /|    NVIDIA GeForce RTX 3090. Num GPUs = 1. Max memory: 23.691 GB. Platform: Linux.\n",
      "O^O/ \\_/ \\    Torch: 2.4.0+cu121. CUDA: 8.6. CUDA Toolkit: 12.1. Triton: 3.0.0\n",
      "\\        /    Bfloat16 = TRUE. FA [Xformers = 0.0.27.post2+cu118. FA2 = False]\n",
      " \"-____-\"     Free license: http://github.com/unslothai/unsloth\n",
      "Unsloth: Fast downloading is enabled - ignore downloading bars which are red colored!\n"
     ]
    }
   ],
   "source": [
    "from unsloth import FastLanguageModel\n",
    "import torch\n",
    "\n",
    "# 저장된 경로 지정\n",
    "save_directory = \"/home/alpaco/chat_bot/wellnis\"\n",
    "\n",
    "# 모델과 토크나이저 불러오기\n",
    "model, tokenizer = FastLanguageModel.from_pretrained(\n",
    "    model_name = save_directory,\n",
    "    max_seq_length = 2048,\n",
    "    dtype = None,\n",
    "    load_in_4bit = True,  # 양자화 옵션을 동일하게 설정\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ...기존 모델/토크나이저 로딩 코드 이후...\n",
    "\n",
    "from transformers import TextStreamer\n",
    "\n",
    "# 1. 원샷 프롬프트 정의\n",
    "one_shot_prompt = \"\"\"\n",
    "### Instruction:\n",
    "너무 무기력해.\n",
    "\n",
    "### Response:\n",
    "증상: 무기력증 무기력감은 삶의 목적과 의미를 잃고, 아무 일도 하지 못하는 상태를 말합니다. 이 증상은 우울증과 함께 나타날 수 있지만, 우울감만으로는 설명이 안되는 경우가 많습니다. 무기력감은 수면 문제, 식욕 변화, 집중력 저하 등의 증상이 동반될 수 있습니다.\n",
    "\n",
    "### Instruction:\n",
    "너무 우울해.\n",
    "\n",
    "### Response:\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<|begin_of_text|>\n",
      "### Instruction:\n",
      "너무 무기력해.\n",
      "\n",
      "### Response:\n",
      "증상: 무기력증 무기력감은 삶의 목적과 의미를 잃고, 아무 일도 하지 못하는 상태를 말합니다. 이 증상은 우울증과 함께 나타날 수 있지만, 우울감만으로는 설명이 안되는 경우가 많습니다. 무기력감은 수면 문제, 식욕 변화, 집중력 저하 등의 증상이 동반될 수 있습니다.\n",
      "\n",
      "### Instruction:\n",
      "너무 우울해.\n",
      "\n",
      "### Response:\n",
      "증상: 우울감 우울감은 우울한 기분이 지속적으로 지속되는 것을 말합니다. 우울감은 신체적 증상(의욕 상실, 식욕 변화, 수면 장애, 피로감 등)이 동반될 수 있습니다.<|end_of_text|>\n"
     ]
    }
   ],
   "source": [
    "# 2. 추론을 위한 입력 준비\n",
    "inputs = tokenizer(\n",
    "    one_shot_prompt,\n",
    "    return_tensors=\"pt\"\n",
    ").to(\"cuda\")\n",
    "\n",
    "# 3. TextStreamer 초기화\n",
    "text_streamer = TextStreamer(tokenizer)\n",
    "\n",
    "# 4. 모델을 사용하여 텍스트 생성 및 스트리밍 출력\n",
    "output = model.generate(\n",
    "    **inputs,\n",
    "    streamer=text_streamer,\n",
    "    max_new_tokens=128,\n",
    "    do_sample=True,\n",
    "    temperature=0.7,\n",
    "    top_p=0.9\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'증상: 우울감 우울감은 우울한 기분이 지속적으로 지속되는 것을 말합니다. 우울감은 신체적 증상(의욕 상실, 식욕 변화, 수면 장애, 피로감 등)이 동반될 수 있습니다.'"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 5. 답변만 추출\n",
    "generated_text = tokenizer.decode(output[0], skip_special_tokens=True)\n",
    "generated_text.split(\"### Response:\")[-1].strip()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "fast_chat",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
