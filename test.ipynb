{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ¦¥ Unsloth: Will patch your computer to enable 2x faster free finetuning.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/alpaco/anaconda3/envs/fast_chat2/lib/python3.11/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ¦¥ Unsloth Zoo will now patch everything to make training faster!\n",
      "==((====))==  Unsloth 2025.5.7: Fast Llama patching. Transformers: 4.51.3.\n",
      "   \\\\   /|    NVIDIA GeForce RTX 3090. Num GPUs = 1. Max memory: 23.691 GB. Platform: Linux.\n",
      "O^O/ \\_/ \\    Torch: 2.4.0+cu121. CUDA: 8.6. CUDA Toolkit: 12.1. Triton: 3.0.0\n",
      "\\        /    Bfloat16 = TRUE. FA [Xformers = 0.0.27.post2+cu118. FA2 = False]\n",
      " \"-____-\"     Free license: http://github.com/unslothai/unsloth\n",
      "Unsloth: Fast downloading is enabled - ignore downloading bars which are red colored!\n"
     ]
    }
   ],
   "source": [
    "# Unsloth ë¼ì´ë¸ŒëŸ¬ë¦¬ì—ì„œ FastLanguageModelì„ ì„í¬íŠ¸\n",
    "from unsloth import FastLanguageModel\n",
    "import torch\n",
    "\n",
    "max_seq_length = 2048 # ìµœëŒ€ ì‹œí€€ìŠ¤ ê¸¸ì´ë¥¼ ì„¤ì • ( í…ìŠ¤íŠ¸ì˜ ìµœëŒ€ ê¸¸ì´ë¥¼ ì§€ì •)\n",
    "dtype = None  # ìë™ ê°ì§€ë¥¼ ìœ„í•´ None ì„¤ì •. Tesla T4ëŠ” Float16, Ampere+ëŠ” Bfloat16 ì‚¬ìš©. ëª¨ë¸ì˜ íŒŒë¼ë¯¸í„°ë¥¼ ì €ì¥í•  ë°ì´í„° íƒ€ì…\n",
    "load_in_4bit = True  # ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰ì„ ì¤„ì´ê¸° ìœ„í•´ 4ë¹„íŠ¸ ì–‘ìí™” ì‚¬ìš©. ë‹¤ë§Œ ì–‘ìí™”ì— ë”°ë¥¸ ì†ì‹¤ì´ ìˆì–´ì„œ í•„ìš”ì— ë”°ë¼ Falseë¡œ ì„¤ì • ê°€ëŠ¥\n",
    "\n",
    "# ì‚¬ì „ í•™ìŠµëœ ëª¨ë¸ê³¼ í† í¬ë‚˜ì´ì € ë¡œë“œ\n",
    "model, tokenizer = FastLanguageModel.from_pretrained(\n",
    "    model_name = \"unsloth/Meta-Llama-3.1-8B\",  # ì‚¬ìš©í•  ëª¨ë¸ ì´ë¦„\n",
    "    max_seq_length = max_seq_length,         # ì„¤ì •í•œ ìµœëŒ€ ì‹œí€€ìŠ¤ ê¸¸ì´\n",
    "    dtype = dtype,                           # ë°ì´í„° íƒ€ì… ì„¤ì •\n",
    "    load_in_4bit = load_in_4bit,             # 4ë¹„íŠ¸ ì–‘ìí™” ì—¬ë¶€\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Unsloth 2025.5.7 patched 32 layers with 32 QKV layers, 32 O layers and 32 MLP layers.\n"
     ]
    }
   ],
   "source": [
    "# PEFT(íŒŒë¼ë¯¸í„° íš¨ìœ¨ì  íŒŒì¸íŠœë‹) ëª¨ë¸ ì„¤ì •\n",
    "model = FastLanguageModel.get_peft_model(\n",
    "    model,\n",
    "    r = 16,  # LoRA ë­í¬ ì„¤ì •. 8, 16, 32, 64, 128 ê¶Œì¥. r ê°’ì´ í´ìˆ˜ë¡ ëª¨ë¸ì´ ë” ë§ì€ ì •ë³´ë¥¼ í•™ìŠµí•  ìˆ˜ ìˆì§€ë§Œ, ë„ˆë¬´ í¬ë©´ ë©”ëª¨ë¦¬ë¥¼ ë§ì´ ì‚¬ìš©\n",
    "    target_modules = [\"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\",\n",
    "                      \"gate_proj\", \"up_proj\", \"down_proj\"],  # PEFT ì ìš©í•  ëª¨ë“ˆ ëª©ë¡. ëª¨ë¸ì˜ íŠ¹ì • ë¶€ë¶„(ëª¨ë“ˆ)ì—ë§Œ í•™ìŠµ\n",
    "    lora_alpha = 16,        # LoRA ì•ŒíŒŒ ì„¤ì • LoRAë¼ëŠ” ê¸°ìˆ ì´ ì–¼ë§ˆë‚˜ ê°•í•˜ê²Œ ì‘ìš©í• ì§€ ì¡°ì ˆ\n",
    "    lora_dropout = 0,       # LoRA ë“œë¡­ì•„ì›ƒ ì„¤ì •. 0ìœ¼ë¡œ ìµœì í™”\n",
    "    bias = \"none\",          # ë°”ì´ì–´ìŠ¤ ì„¤ì •. \"none\"ìœ¼ë¡œ ìµœì í™”\n",
    "\n",
    "    # \"unsloth\" ì‚¬ìš© ì‹œ VRAM ì ˆì•½ ë° ë°°ì¹˜ ì‚¬ì´ì¦ˆ 2ë°° ì¦ê°€\n",
    "    # í•™ìŠµí•  ë•Œ ë©”ëª¨ë¦¬ë¥¼ ì ˆì•½í•˜ëŠ” ë°©ë²•ì„ ì‚¬ìš©í•˜ëŠ” ì„¤ì •\n",
    "    use_gradient_checkpointing = \"unsloth\",  # ë§¤ìš° ê¸´ ì»¨í…ìŠ¤íŠ¸ë¥¼ ìœ„í•´ \"unsloth\" ì„¤ì •\n",
    "    random_state = 3407,    # ëœë¤ ì‹œë“œ ì„¤ì •\n",
    "    use_rslora = False,     # ë­í¬ ì•ˆì •í™” LoRA ì‚¬ìš© ì—¬ë¶€\n",
    "    loftq_config = None,    # LoftQ ì„¤ì • (ì‚¬ìš©í•˜ì§€ ì•ŠìŒ)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'<|end_of_text|>'"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# ëª¨ë¸ì—ê²Œ ì£¼ì–´ì§ˆ í…ìŠ¤íŠ¸ì˜ í˜•ì‹ì„ ì •ì˜\n",
    "alpaca_prompt = \"\"\"Below is an instruction that describes a task. Write a response that appropriately completes the request.\n",
    "\n",
    "### Instruction:\n",
    "{}\n",
    "\n",
    "### Response:\n",
    "{}\"\"\"\n",
    "\n",
    "# ë§¨ìœ—ì¤„: ëª¨ë¸ì—ê²Œ ì•ìœ¼ë¡œ ì œê³µë  ì§€ì‹œì‚¬í•­ì„ ê¸°ë°˜ìœ¼ë¡œ ì ì ˆí•œ ì‘ë‹µì„ ì‘ì„±í•˜ë¼ê³  ë§í•¨.\n",
    "# ëª¨ë¸ì´ ì–´ë–¤ ì‘ì—…ì„ ìˆ˜í–‰í•´ì•¼ í•˜ëŠ”ì§€ ëª…í™•íˆ ì´í•´í•  ìˆ˜ ìˆë„ë¡ ë„ì™€ì¤Œ\n",
    "# Instruction:ì€ ì§€ì‹œì‚¬í•­ì´ ì œê³µë  ë¶€ë¶„\n",
    "# Response:ëŠ” ëª¨ë¸ì´ ìƒì„±í•´ì•¼ í•  ì‘ë‹µì´ ì œê³µë  ë¶€ë¶„\n",
    "\n",
    "# EOS í† í° ê°€ì ¸ì˜¤ê¸° (ìƒì„± ì¢…ë£Œë¥¼ ìœ„í•´ í•„ìš”)\n",
    "EOS_TOKEN = tokenizer.eos_token  # ë°˜ë“œì‹œ EOS_TOKENì„ ì¶”ê°€í•´ì•¼ í•¨\n",
    "EOS_TOKEN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generating train split: 857 examples [00:00, 37683.53 examples/s]\n",
      "Map: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 857/857 [00:00<00:00, 72164.60 examples/s]\n"
     ]
    }
   ],
   "source": [
    "# í”„ë¡¬í”„íŠ¸ í¬ë§·íŒ… í•¨ìˆ˜ ì •ì˜\n",
    "def formatting_prompts_func(examples):\n",
    "    instructions = examples[\"ìœ ì €\"]  # ë°ì´í„°ì…‹ì˜ 'instruction' í•„ë“œ\n",
    "    outputs      = examples[\"ì±—ë´‡\"]       # ë°ì´í„°ì…‹ì˜ 'output' í•„ë“œ\n",
    "    texts = []\n",
    "    for instruction, output in zip(instructions, outputs):\n",
    "                                                           # EOS_TOKENì„ ì¶”ê°€í•˜ì§€ ì•Šìœ¼ë©´ ìƒì„±ì´ ë¬´í•œíˆ ê³„ì†ë¨\n",
    "        text = alpaca_prompt.format(instruction, output) + EOS_TOKEN  # í”„ë¡¬í”„íŠ¸ í˜•ì‹ì— ë§ê²Œ í…ìŠ¤íŠ¸ ìƒì„±\n",
    "        texts.append(text)\n",
    "    return { \"text\" : texts, }  # 'text' í•„ë“œë¡œ ë°˜í™˜\n",
    "\n",
    "#from datasets import load_dataset  # Hugging Face datasets ë¼ì´ë¸ŒëŸ¬ë¦¬ ì„í¬íŠ¸\n",
    "\n",
    "# ë°ì´í„°ì…‹ ë¡œë“œ (Teddy Leeì˜ QA ë°ì´í„°ì…‹ ë¯¸ë‹ˆ ë²„ì „)\n",
    "#dataset = load_dataset(\"teddylee777/QA-Dataset-mini\", split = \"train\")\n",
    "\n",
    "# í”„ë¡¬í”„íŠ¸ í¬ë§·íŒ… í•¨ìˆ˜ ì ìš©í•˜ì—¬ ë°ì´í„°ì…‹ ë³€í™˜\n",
    "#dataset = dataset.map(formatting_prompts_func, batched = True,)\n",
    "\n",
    "from datasets import load_dataset\n",
    "dataset = load_dataset( \"csv\", data_files = \"/home/alpaco/chat_bot/wellnis.csv\", split = \"train\")\n",
    "dataset = dataset.map(formatting_prompts_func, batched=True)\n",
    "\n",
    "# ì˜ˆì œ ì—‘ì…€ í˜•íƒœë¡œ ë°ì´í„°ì…‹ì„ ë§Œë“ ë‹¤ë©´?\n",
    "# from datasets import load_dataset\n",
    "# dataset = load_dataset( \"csv\", data_files = \"data.csv\", split = \"train\")\n",
    "# dataset = dataset.map(formatting_prompts_func, batched=True)\n",
    "#\n",
    "#"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Below is an instruction that describes a task. Write a response that appropriately completes the request.\\n\\n### Instruction:\\në‹¤ ì œ ì˜ëª»ì´ì—ìš”.\\n\\n### Response:\\në‚šì‹œë¥¼ í•˜ë“¯ì´ ì¡°ê¸ˆ ë” ë¨¼ ê³³ì„ ë³´ê³ , ì¡°ê¸ˆ ë” ë§ˆìŒì„ ë¹„ìš°ë©´ í¸ì•ˆí•´ì§ˆ ê±°ì˜ˆìš”.<|end_of_text|>',\n",
       " 'Below is an instruction that describes a task. Write a response that appropriately completes the request.\\n\\n### Instruction:\\nì–´ë ¸ì„ ë•Œë¶€í„° ì—„ë§ˆë³´ë‹¤ëŠ” ì•„ë¹ ë‘ ë” ì¹œí–ˆì–´ìš”.\\n\\n### Response:\\nê·¸ëŸ¬ì‹œêµ°ìš”. ì•„ë²„ì§€ì— ëŒ€í•´ ë” ë“¤ë ¤ì£¼ì„¸ìš”.<|end_of_text|>',\n",
       " 'Below is an instruction that describes a task. Write a response that appropriately completes the request.\\n\\n### Instruction:\\nëª¸ë¬´ê²Œë„ ëŠ˜ê³ ìš”.\\n\\n### Response:\\nì²´ì¤‘ì´ ë‹¨ê¸°ê°„ì— ë§ì´ ëŠ˜ì–´ë‚˜ë©´ ê±´ê°•ì— ì¢‹ì§€ ì•Šë‹¤ê³  í•´ìš”.<|end_of_text|>',\n",
       " 'Below is an instruction that describes a task. Write a response that appropriately completes the request.\\n\\n### Instruction:\\nëˆˆë¬¼ì´ ë‚  ê±° ê°™ë”ë¼ê³ .\\n\\n### Response:\\nê°ìˆ˜ì„±ì´ í’ë¶€í•˜ë‹¤ëŠ” ì¦ê±° ê°™ì•„ìš”. ëˆˆë¬¼ì´ ë§ì€ ì‚¬ëŒ ì¤‘ì— ë‚˜ìœ ì‚¬ëŒì€ ì—†ì–ì•„ìš”.<|end_of_text|>',\n",
       " 'Below is an instruction that describes a task. Write a response that appropriately completes the request.\\n\\n### Instruction:\\níšŒì‚¬ë¥¼ ì˜®ê¸°ë©´ì„œ ì§ê¸‰ë„ ì˜¬ë¼ì„œ, ì§€ê¸ˆì€ ê³¼ì¥ì´ì—ìš”.\\n\\n### Response:\\nê·¸ë ‡êµ°ìš”. ìƒˆë¡œìš´ í™˜ê²½ì´ë¼ ì ì‘ì´ í•„ìš”í•˜ê² ì–´ìš”.<|end_of_text|>']"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset['text'][:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Unsloth: Tokenizing [\"text\"] (num_proc=2): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 857/857 [00:01<00:00, 539.02 examples/s]\n",
      "==((====))==  Unsloth - 2x faster free finetuning | Num GPUs used = 1\n",
      "   \\\\   /|    Num examples = 857 | Num Epochs = 1 | Total steps = 60\n",
      "O^O/ \\_/ \\    Batch size per device = 2 | Gradient accumulation steps = 4\n",
      "\\        /    Data Parallel GPUs = 1 | Total batch size (2 x 4 x 1) = 8\n",
      " \"-____-\"     Trainable parameters = 41,943,040/8,000,000,000 (0.52% trained)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='60' max='60' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [60/60 01:25, Epoch 0/1]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>3.274200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>3.394200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>3.408800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>3.288600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>3.069200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6</td>\n",
       "      <td>2.557600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7</td>\n",
       "      <td>2.456700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8</td>\n",
       "      <td>2.317200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9</td>\n",
       "      <td>2.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10</td>\n",
       "      <td>1.810600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>11</td>\n",
       "      <td>1.776400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>12</td>\n",
       "      <td>1.719400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>13</td>\n",
       "      <td>1.723100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>14</td>\n",
       "      <td>1.628200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>15</td>\n",
       "      <td>1.644300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>16</td>\n",
       "      <td>1.730100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>17</td>\n",
       "      <td>1.782100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>18</td>\n",
       "      <td>1.506400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>19</td>\n",
       "      <td>1.660700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>20</td>\n",
       "      <td>1.510300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>21</td>\n",
       "      <td>1.627300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>22</td>\n",
       "      <td>1.469800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>23</td>\n",
       "      <td>1.422100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>24</td>\n",
       "      <td>1.594500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>25</td>\n",
       "      <td>1.455000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>26</td>\n",
       "      <td>1.478300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>27</td>\n",
       "      <td>1.359900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>28</td>\n",
       "      <td>1.367100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>29</td>\n",
       "      <td>1.532700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>30</td>\n",
       "      <td>1.424300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>31</td>\n",
       "      <td>1.551600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>32</td>\n",
       "      <td>1.428300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>33</td>\n",
       "      <td>1.492900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>34</td>\n",
       "      <td>1.542600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>35</td>\n",
       "      <td>1.612400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>36</td>\n",
       "      <td>1.471100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>37</td>\n",
       "      <td>1.550500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>38</td>\n",
       "      <td>1.544800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>39</td>\n",
       "      <td>1.444400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>40</td>\n",
       "      <td>1.682000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>41</td>\n",
       "      <td>1.434800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>42</td>\n",
       "      <td>1.462900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>43</td>\n",
       "      <td>1.565900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>44</td>\n",
       "      <td>1.471200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>45</td>\n",
       "      <td>1.377900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>46</td>\n",
       "      <td>1.392300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>47</td>\n",
       "      <td>1.416100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>48</td>\n",
       "      <td>1.359600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>49</td>\n",
       "      <td>1.590900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>50</td>\n",
       "      <td>1.374200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>51</td>\n",
       "      <td>1.360100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>52</td>\n",
       "      <td>1.432100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>53</td>\n",
       "      <td>1.504700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>54</td>\n",
       "      <td>1.488200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>55</td>\n",
       "      <td>1.470900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>56</td>\n",
       "      <td>1.383900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>57</td>\n",
       "      <td>1.558900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>58</td>\n",
       "      <td>1.432800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>59</td>\n",
       "      <td>1.509600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>60</td>\n",
       "      <td>1.446500</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "## í•™ìŠµ ì„¤ì •\n",
    "from trl import SFTTrainer  # TRL ë¼ì´ë¸ŒëŸ¬ë¦¬ì—ì„œ SFTTrainer ì„í¬íŠ¸\n",
    "from transformers import TrainingArguments  # íŠ¸ëœìŠ¤í¬ë¨¸ ë¼ì´ë¸ŒëŸ¬ë¦¬ì—ì„œ TrainingArguments ì„í¬íŠ¸\n",
    "from unsloth import is_bfloat16_supported  # BFloat16 ì§€ì› ì—¬ë¶€ í™•ì¸ í•¨ìˆ˜ ì„í¬íŠ¸\n",
    "\n",
    "# SFTTrainer ì¸ìŠ¤í„´ìŠ¤ ìƒì„±\n",
    "trainer = SFTTrainer(\n",
    "    model = model,                           # í•™ìŠµí•  ëª¨ë¸\n",
    "    tokenizer = tokenizer,                   # ì‚¬ìš©í•  í† í¬ë‚˜ì´ì €\n",
    "    train_dataset = dataset,                 # í•™ìŠµí•  ë°ì´í„°ì…‹ â˜…â˜…â˜…â˜…â˜…â˜…â˜…â˜…\n",
    "    dataset_text_field = \"text\",             # ë°ì´í„°ì…‹ì˜ í…ìŠ¤íŠ¸ í•„ë“œ ì´ë¦„ â˜…â˜…â˜…â˜…â˜…â˜…â˜…â˜…\n",
    "    max_seq_length = max_seq_length,         # ìµœëŒ€ ì‹œí€€ìŠ¤ ê¸¸ì´\n",
    "    dataset_num_proc = 2,                    # ë°ì´í„°ì…‹ ì „ì²˜ë¦¬ì— ì‚¬ìš©í•  í”„ë¡œì„¸ìŠ¤ ìˆ˜ cpu\n",
    "    packing = False,                         # ì§§ì€ ì‹œí€€ìŠ¤ì˜ ê²½ìš° packingì„ ë¹„í™œì„±í™” (í•™ìŠµ ì†ë„ 5ë°° í–¥ìƒ ê°€ëŠ¥)\n",
    "    args = TrainingArguments(\n",
    "        per_device_train_batch_size = 2,     # ë””ë°”ì´ìŠ¤ ë‹¹ ë°°ì¹˜ ì‚¬ì´ì¦ˆ\n",
    "        gradient_accumulation_steps = 4,     # ê·¸ë˜ë””ì–¸íŠ¸ ëˆ„ì  ë‹¨ê³„ ìˆ˜\n",
    "        warmup_steps = 5,                     # ì›Œë°ì—… ìŠ¤í… ìˆ˜\n",
    "        # num_train_epochs = 1,               # ì „ì²´ í•™ìŠµ ì—í­ ìˆ˜ ì„¤ì • ê°€ëŠ¥\n",
    "        max_steps = 60,                       # ìµœëŒ€ í•™ìŠµ ìŠ¤í… ìˆ˜\n",
    "        learning_rate = 2e-4,                 # í•™ìŠµë¥ \n",
    "        fp16 = not is_bfloat16_supported(),   # BFloat16 ì§€ì› ì—¬ë¶€ì— ë”°ë¼ FP16 ì‚¬ìš©\n",
    "        bf16 = is_bfloat16_supported(),       # BFloat16 ì‚¬ìš© ì—¬ë¶€\n",
    "        logging_steps = 1,                    # ë¡œê¹… ë¹ˆë„\n",
    "        optim = \"adamw_8bit\",                  # ì˜µí‹°ë§ˆì´ì € ì„¤ì • (8ë¹„íŠ¸ AdamW)\n",
    "        weight_decay = 0.01,                  # ê°€ì¤‘ì¹˜ ê°ì‡ \n",
    "        lr_scheduler_type = \"linear\",         # í•™ìŠµë¥  ìŠ¤ì¼€ì¤„ëŸ¬ íƒ€ì…\n",
    "        seed = 3407,                           # ëœë¤ ì‹œë“œ ì„¤ì •\n",
    "        output_dir = \"outputs\",                # ì¶œë ¥ ë””ë ‰í† ë¦¬\n",
    "    ),\n",
    ")\n",
    "\n",
    "## í•™ìŠµ ì‹¤í–‰\n",
    "trainer_stats = trainer.train()  # ëª¨ë¸ í•™ìŠµ ì‹œì‘ # 3d6e4b9a0ddcd9edfafad59dcde0fd8c666954fd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('wellnis/tokenizer_config.json',\n",
       " 'wellnis/special_tokens_map.json',\n",
       " 'wellnis/tokenizer.json')"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# ëª¨ë¸ ì €ì¥ ë¡œì»¬í´ë”ì—ë‹¤ê°€ ì €ì¥í•˜ëŠ” ë°©ì‹\n",
    "model.save_pretrained(\"wellnis\")  # Local saving\n",
    "tokenizer.save_pretrained(\"wellnis\")\n",
    "\n",
    "# ì´ ì´ì™¸ì— í—ˆê¹…í˜ì´ìŠ¤ë‚˜ ë‹¤ë¥¸ hubì— pushí•´ì„œ ì €ì¥í•˜ëŠ” ë°©ë²•ì´ ìˆìŒ\n",
    "# ë‹¤ë§Œ, ì—…ë¡œë“œ ì†ë„ì™€ ë‹¤ìš´ë¡œë“œ ì†ë„ë¥¼ ê³ ë ¤í•´ì•¼í•¨."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==((====))==  Unsloth 2025.5.7: Fast Llama patching. Transformers: 4.51.3.\n",
      "   \\\\   /|    NVIDIA GeForce RTX 3090. Num GPUs = 1. Max memory: 23.691 GB. Platform: Linux.\n",
      "O^O/ \\_/ \\    Torch: 2.4.0+cu121. CUDA: 8.6. CUDA Toolkit: 12.1. Triton: 3.0.0\n",
      "\\        /    Bfloat16 = TRUE. FA [Xformers = 0.0.27.post2+cu118. FA2 = False]\n",
      " \"-____-\"     Free license: http://github.com/unslothai/unsloth\n",
      "Unsloth: Fast downloading is enabled - ignore downloading bars which are red colored!\n"
     ]
    }
   ],
   "source": [
    "from unsloth import FastLanguageModel\n",
    "import torch\n",
    "\n",
    "# ì €ì¥ëœ ê²½ë¡œ ì§€ì •\n",
    "save_directory = \"/home/alpaco/chat_bot/wellnis\"\n",
    "\n",
    "# ëª¨ë¸ê³¼ í† í¬ë‚˜ì´ì € ë¶ˆëŸ¬ì˜¤ê¸°\n",
    "model, tokenizer = FastLanguageModel.from_pretrained(\n",
    "    model_name = save_directory,\n",
    "    max_seq_length = 2048,\n",
    "    dtype = None,\n",
    "    load_in_4bit = True,  # ì–‘ìí™” ì˜µì…˜ì„ ë™ì¼í•˜ê²Œ ì„¤ì •\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<|begin_of_text|>Below is an instruction that describes a task. Write a response that appropriately completes the request.\n",
      "\n",
      "### Instruction:\n",
      "ë„ˆë¬´ ìš°ìš¸í•´.\n",
      "\n",
      "### Response:\n",
      "ê·¸ëŸ´ ë•ŒëŠ” í˜ë“¤ì£ .<|end_of_text|>\n"
     ]
    }
   ],
   "source": [
    "# ì¶”ë¡ í•´ë³´ê¸°\n",
    "FastLanguageModel.for_inference(model)  # ë„¤ì´í‹°ë¸Œ 2ë°° ë¹ ë¥¸ ì¶”ë¡  í™œì„±í™”\n",
    "\n",
    "# ì¶”ë¡ ì„ ìœ„í•œ ì…ë ¥ ì¤€ë¹„\n",
    "inputs = tokenizer(\n",
    "[\n",
    "    alpaca_prompt.format(\n",
    "        \"ë„ˆë¬´ ìš°ìš¸í•´.\", # ì¸ìŠ¤íŠ¸ëŸ­ì…˜ (ëª…ë ¹ì–´)\n",
    "        \"\", # ì¶œë ¥ - ìƒì„±í•  ë‹µë³€ì„ ë¹„ì›Œë‘ \n",
    "    )\n",
    "], return_tensors = \"pt\").to(\"cuda\")  # í…ì„œë¥¼ PyTorch í˜•ì‹ìœ¼ë¡œ ë³€í™˜í•˜ê³  GPUë¡œ ì´ë™\n",
    "\n",
    "from transformers import TextStreamer  # í…ìŠ¤íŠ¸ ìŠ¤íŠ¸ë¦¬ë°ì„ ìœ„í•œ TextStreamer ì„í¬íŠ¸\n",
    "\n",
    "text_streamer = TextStreamer(tokenizer)  # í† í¬ë‚˜ì´ì €ë¥¼ ì‚¬ìš©í•˜ì—¬ ìŠ¤íŠ¸ë¦¬ë¨¸ ì´ˆê¸°í™”\n",
    "\n",
    "# ëª¨ë¸ì„ ì‚¬ìš©í•˜ì—¬ í…ìŠ¤íŠ¸ ìƒì„± ë° ìŠ¤íŠ¸ë¦¬ë° ì¶œë ¥\n",
    "_ = model.generate(**inputs, streamer = text_streamer, max_new_tokens = 128)  # ìµœëŒ€ 128ê°œì˜ ìƒˆë¡œìš´ í† í° ìƒì„±"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'ê·¸ëŸ´ ë•ŒëŠ” í˜ë“¤ì£ .'"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# ë‹µë³€ë§Œ ì¶”ì¶œ\n",
    "generated_text = tokenizer.decode(_[0], skip_special_tokens=True)\n",
    "generated_text.split(\"### Response:\")[1].strip()"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
